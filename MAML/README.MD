Readme 
# Anemia Few-Shot Regression with MAML

This notebook (Anemia_regression_V1-3.ipynb) implements a *few-shot, first-order MAML* model to predict hemoglobin (Hb) from conjunctiva images.

This README is written for someone opening the notebook for the first time. It explains:

- Where to set *data paths*
- What *dataset structure* is expected
- How the *MAML code* is organized and how it works
- How to *run* the notebook end-to-end

---

## 1. Data & Folder Structure

### 1.1. Base dataset directory

Early in the notebook you will see:

```python
## Set Dataset Paths

# Define dataset directories
BASE_DIR  = Path("/content/drive/MyDrive/Anemia_dataset")
INDIA_DIR = BASE_DIR / "India"
ITALY_DIR = BASE_DIR / "Italy"

# Quick existence check
print("Base dir exists:", BASE_DIR.exists())
print("India dir exists:", INDIA_DIR.exists())
print("Italy dir exists:", ITALY_DIR.exists())
üëâ This is the main place where you need to change the path.
	‚Ä¢	If you‚Äôre not using Google Drive / Colab, change BASE_DIR to your local or server path, e.g.:‚Ä®BASE_DIR = Path("/home/username/data/Anemia_dataset")
	‚Ä¢	
	‚Ä¢	INDIA_DIR and ITALY_DIR are derived automatically from BASE_DIR.‚Ä®Keep their definitions as they are unless your folder names differ.
1.2. Expected directory layout
The notebook expects something like:
Anemia_dataset/
‚îú‚îÄ‚îÄ India/
‚îÇ   ‚îú‚îÄ‚îÄ India.xlsx          # metadata for Indian patients
‚îÇ   ‚îú‚îÄ‚îÄ ... image folders / files ...
‚îú‚îÄ‚îÄ Italy/
‚îÇ   ‚îú‚îÄ‚îÄ Italy.xlsx          # metadata for Italian patients
‚îÇ   ‚îú‚îÄ‚îÄ ... image folders / files ...
‚îî‚îÄ‚îÄ Augmented_Data/
    ‚îú‚îÄ‚îÄ India/
    ‚îÇ   ‚îú‚îÄ‚îÄ ... augmented images ...
    ‚îî‚îÄ‚îÄ Italy/
        ‚îú‚îÄ‚îÄ ... augmented images (optional) ...
Later, augmented data paths are defined as:
## Set Augmented Dataset Paths

AUG_BASE      = BASE_DIR / "Augmented_Data"
AUG_INDIA_DIR = AUG_BASE / "India"
AUG_ITALY_DIR = AUG_BASE / "Italy"

print("Aug base exists:", AUG_BASE.exists())
print("Aug India exists:", AUG_INDIA_DIR.exists())
print("Aug Italy exists:", AUG_ITALY_DIR.exists())
If you don‚Äôt have augmented data yet, you can:
	‚Ä¢	Either create the above structure
	‚Ä¢	Or temporarily set use_augmented=False when building image tables (already done for Italy; you can mirror that behavior for India if needed).
1.3. Metadata files
Two main Excel files are used:
# India metadata
INDIA_XLSX = INDIA_DIR / "India.xlsx"
df_india = pd.read_excel(INDIA_XLSX)

# Italy metadata
ITALY_XLSX = ITALY_DIR / "Italy.xlsx"
df_italy = pd.read_excel(ITALY_XLSX)
These Excel files are expected to contain at least:
	‚Ä¢	Number ‚Üí patient ID
	‚Ä¢	Label ‚Üí anemia / non-anemia class
	‚Ä¢	Hgb ‚Üí hemoglobin value
If your column names differ, you‚Äôll need to update the relevant functions (e.g. build_rg_df) to match your column names.

2. Environment & Dependencies
The notebook uses:
	‚Ä¢	Python 3
	‚Ä¢	PyTorch
	‚Ä¢	NumPy
	‚Ä¢	pandas
	‚Ä¢	scikit-learn
	‚Ä¢	OpenCV (cv2)
	‚Ä¢	matplotlib
	‚Ä¢	(optionally) GPU (CUDA) for faster training
In Colab, most of these are available by default; any missing libs are usually installed with pip at the top of the notebook.
If running locally, install dependencies with e.g.:
pip install torch torchvision torchaudio \
            numpy pandas scikit-learn opencv-python matplotlib openpyxl

3. Notebook Structure Overview
At a high level, the notebook does:
	1.	Mount Drive (optional / Colab only) and import all dependencies.
	2.	Set dataset paths (BASE_DIR, INDIA_DIR, ITALY_DIR).
	3.	Load metadata from India and Italy Excel files.
	4.	Build an R/G ratio dataset for each patient (feature engineering).
	5.	Create patient-level tables and splits (train / val / test).
	6.	Expand to image-level tables, including augmented images.
	7.	Define PyTorch dataset and episodic sampler for few-shot learning.
	8.	Define the few-shot CNN model (FewShotConvNet).
	9.	Define inner-loop adaptation (inner_adapt_params) for MAML.
	10.	Define MAML training loop (train_maml) with validation-based early stopping.
	11.	Train the MAML model on India train episodes.
	12.	Evaluate on India validation and Italy test splits using episodic evaluation and compute regression + classification metrics.
The rest of this README explains the MAML-related parts in more detail.

4. Data Processing & Episode Creation
4.1. R/G ratio data (build_rg_df)
The function:
def build_rg_df(df_meta, base_dir, id_col="Number", hb_col="Hgb", label_col="Label"):
    ...
	‚Ä¢	Iterates over each patient in the metadata.
	‚Ä¢	Locates a conjunctiva image for that patient under base_dir.
	‚Ä¢	Computes a simple R/G ratio as a color feature.
	‚Ä¢	Builds a dataframe with:
	‚Ä¢	patient_id
	‚Ä¢	hb (hemoglobin)
	‚Ä¢	label (anemia/non-anemia)
	‚Ä¢	image_path
	‚Ä¢	rg_ratio
You generally don‚Äôt need to edit this function unless:
	‚Ä¢	Your image filenames / subdirectory structure is very different, or
	‚Ä¢	Your metadata column names differ and you aren‚Äôt using the id_col / hb_col / label_col arguments.
4.2. Patient-level tables and splits
For India:
df_india_patients = (
    df_india[["Number", "Label", "Hgb"]]
    .drop_duplicates()
    .rename(columns={"Number": "patient_id", "Hgb": "hb"})
)

# Attach split: "train" / "val"
df_india_patients["split"] = np.where(
    df_india_patients["patient_id"].isin(train_patients),
    "train",
    "val",
)
For Italy, a similar patient-level table is built and assigned primarily to a test split.
Here, train_patients etc. are typically determined by some split logic (e.g. random split or specified by you). Each patient is assigned to exactly one split.
4.3. Image-level tables with augmentation
Images are listed via:
df_images_india = build_image_table_from_augmented(
    df_india_patients,
    AUG_INDIA_DIR,
    country_name="India",
)

df_images_italy = build_image_table_from_augmented(
    df_italy_patients,
    AUG_ITALY_DIR,
    country_name="Italy",
    use_augmented=False,  # Italy uses only original images
)
build_image_table_from_augmented:
	‚Ä¢	For each patient and split, it finds:
	‚Ä¢	The original conjunctiva image.
	‚Ä¢	Any augmented versions (in subfolders or with a naming pattern under AUG_* dirs).
	‚Ä¢	Returns a dataframe with one row per image:
	‚Ä¢	patient_id
	‚Ä¢	split
	‚Ä¢	label_str (e.g. "anemic"/"non-anemic")
	‚Ä¢	hb
	‚Ä¢	image_path
	‚Ä¢	source ("original" or "augmented")
	‚Ä¢	country
Then:
df_images = pd.concat([df_images_india, df_images_italy])
This gives a single image-level dataframe covering both countries.

5. Dataset & EpisodeSampler (Core for MAML)
5.1. PyTorch dataset: ConjunctivaImageDataset
img_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Resize((128, 128)),
])

class ConjunctivaImageDataset(Dataset):
    """
    PyTorch dataset for conjunctiva images (original + augmented).
    Returns:
        - img_tensor: [C, H, W]
        - hb: float
        - patient_id: int
    """
    ...
	‚Ä¢	It reads each image using OpenCV (cv2.imread(row["image_path"])).
	‚Ä¢	Converts BGR ‚Üí RGB, then applies img_transform.
	‚Ä¢	Returns:
	‚Ä¢	The image tensor
	‚Ä¢	The regression target (hb)
	‚Ä¢	The patient_id (for constructing episodes by patient).
The dataset is instantiated as:
df_train = df_images[df_images["split"] == "train"].reset_index(drop=True)
df_val   = df_images[df_images["split"] == "val"].reset_index(drop=True)
df_test  = df_images[df_images["split"] == "test"].reset_index(drop=True)

train_dataset = ConjunctivaImageDataset(df_train, transform=img_transform)
val_dataset   = ConjunctivaImageDataset(df_val,   transform=img_transform)
test_dataset  = ConjunctivaImageDataset(df_test,  transform=img_transform)
5.2. EpisodeSampler
class EpisodeSampler:
    """
    Sampler that builds patient-level few-shot episodes.
    """
    def _init_(self, df_split):
        self.df = df_split.reset_index(drop=True)

        self.patient_label_to_indices = defaultdict(list)
        self.label_to_patients        = defaultdict(set)

        for idx, row in self.df.iterrows():
            pid   = int(row["patient_id"])
            label = int(row["label"])
            self.patient_label_to_indices[(pid, label)].append(idx)
            self.label_to_patients[label].add(pid)
    ...
Core idea:
	‚Ä¢	Organizes indices by (patient_id, label).
	‚Ä¢	Creates episodes by:
	‚Ä¢	Sampling a small number of patients and images as the support set (K-shot).
	‚Ä¢	Sampling additional images from possibly the same or different patients as the query set.
	‚Ä¢	Optionally controlling how often support/query come from the same patient using p_same.
This sampler is used:
train_sampler = EpisodeSampler(df_train)
val_sampler   = EpisodeSampler(df_val)
test_sampler  = EpisodeSampler(df_test)

6. Few-Shot Model (Backbone for MAML)
6.1. Encoder & regressor
You‚Äôll see:
def build_fewshot_encoder():
    ...
    # Returns a CNN producing feature maps [B, 256, 8, 8]
    return encoder

def build_fewshot_regressor():
    return nn.Linear(256, 1)
6.2. Full model: FewShotConvNet
class FewShotConvNet(nn.Module):
    """
    CNN backbone for few-shot hemoglobin regression.
    Input:  [B, 3, 128, 128]
    Output: [B] continuous Hb predictions
    """

    def _init_(self):
        super()._init_()
        self.encoder   = build_fewshot_encoder()
        self.regressor = build_fewshot_regressor()

    def forward(self, x):
        feats = self.encoder(x)          # [B, 256, 8, 8]
        feats = feats.mean(dim=[2, 3])   # Global Average Pool ‚Üí [B, 256]
        out   = self.regressor(feats)    # [B, 1]
        return out.squeeze(1)            # [B]
So the model is:
	‚Ä¢	CNN encoder ‚Üí global average pooling ‚Üí linear regressor ‚Üí scalar Hb.
MAML will adapt all these parameters in the inner loop and learn good initial parameters in the outer loop.

7. Inner-Loop Adaptation (First-Order MAML)
The core inner-loop function is:
def inner_adapt_params(
    model,
    base_params,
    x_support,
    y_support,
    inner_lr=1e-2,
    inner_steps=1,
):
    """
    Perform inner-loop adaptation using the SUPPORT set (first-order MAML).
    Args:
        model       : FewShotConvNet instance
        base_params : dict(name -> tensor) of meta-parameters
        x_support   : [N_s, 3, H, W]
        y_support   : [N_s]
    """
    fast_params = {name: p.clone().detach().requires_grad_(True)
                   for name, p in base_params.items()}

    for step in range(inner_steps):
        # Forward using fast parameters
        y_pred = functional_forward(model, x_support, fast_params)
        loss   = mse_loss(y_pred, y_support)

        grads = torch.autograd.grad(
            loss,
            list(fast_params.values()),
            create_graph=False,  # first-order MAML
        )

        fast_params = {
            name: param - inner_lr * g
            for (name, param), g in zip(fast_params.items(), grads)
        }

    return fast_params
Conceptually:
	1.	Start from the current meta-parameters (base_params).
	2.	Clone them into fast_params (task-specific parameters).
	3.	For a small number of inner steps:
	‚Ä¢	Compute predictions on the support set.
	‚Ä¢	Compute MSE loss against y_support.
	‚Ä¢	Compute gradients w.r.t. fast_params.
	‚Ä¢	Do gradient descent update:‚Ä®fast_params ‚Üê fast_params ‚àí inner_lr ¬∑ grad.
	4.	Return adapted parameters for this specific episode.
Note: A helper functional_forward is used to run the model with arbitrary parameter dicts (typical in functional MAML implementations).

8. MAML Training Loop (train_maml)
8.1. Outer-loop function
def train_maml(
    model,
    optimizer,
    dataset,
    sampler,
    n_epochs=10,
    episodes_per_epoch=100,
    K=1,
    n_query_per_class=5,
    p_same=0.3,
    inner_lr=1e-2,
    inner_steps=1,
    device="cpu",
    # Validation configuration
    val_dataset=None,
    val_sampler=None,
    val_K=1,
    val_p_same=0.3,
    val_n_episodes=50,
):
    ...
High-level behavior:
	1.	For each epoch:
	‚Ä¢	Set model to train().
	‚Ä¢	For episodes_per_epoch episodes:
	1.	Use sampler to sample support and query indices (s_idx, q_idx) with K-shot, p_same, etc.
	2.	Build x_support, y_support, x_query, y_query from dataset.
	3.	Take a snapshot of current model parameters: base_params.
	4.	Run inner adaptation:‚Ä®fast_params = inner_adapt_params(model, base_params, x_support, y_support, ...).
	5.	Compute predictions on x_query using fast_params ‚Üí query loss (mse_loss).
	6.	Accumulate the query losses across episodes as the meta-loss.
	‚Ä¢	After some number of episodes or at the end of the epoch, backprop through the accumulated meta-loss and update the meta-parameters (model.parameters()) using optimizer.step().
	2.	Metrics:
	‚Ä¢	For each epoch, track:
	‚Ä¢	Mean training loss
	‚Ä¢	Mean MAE on the query sets (per episode)
	‚Ä¢	Optionally track counts of "same_patient" vs "different_patient" episodes for debugging.
	3.	Validation & early stopping:
	‚Ä¢	After each epoch, optionally run episodic evaluation on the validation split (val_dataset, val_sampler) using a helper function (e.g. run_episodic_eval).
	‚Ä¢	Compute MAE / RMSE / R¬≤ and store the best meta-parameters (lowest validation MAE).
	‚Ä¢	At the end, return:
	‚Ä¢	A history dict of training / validation metrics.
	‚Ä¢	best_state_dict (to be loaded back into the model).
8.2. Training call
Later in the notebook:
device = "cuda" if torch.cuda.is_available() else "cpu"

model = FewShotConvNet().to(device)
meta_optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

history, best_state_dict = train_maml(
    model=model,
    optimizer=meta_optimizer,
    dataset=train_dataset,
    sampler=train_sampler,
    n_epochs=5,
    episodes_per_epoch=100,
    K=1,
    n_query_per_class=5,
    p_same=0.3,
    inner_lr=1e-2,
    inner_steps=1,
    device=device,
    val_dataset=val_dataset,
    val_sampler=val_sampler,
    val_K=1,
    val_p_same=0.3,
    val_n_episodes=50,
)

if best_state_dict is not None:
    model.load_state_dict(best_state_dict)
You can tweak:
	‚Ä¢	n_epochs ‚Äî total training epochs.
	‚Ä¢	episodes_per_epoch ‚Äî how many episodes per epoch.
	‚Ä¢	K ‚Äî shots per class / patient for the support set.
	‚Ä¢	inner_steps and inner_lr ‚Äî how aggressively you adapt in the inner loop.
	‚Ä¢	p_same ‚Äî probability that support & query come from the same patient (controls task difficulty).

9. Evaluation & Metrics
After training, the notebook runs episodic evaluation on:
	‚Ä¢	India validation set
	‚Ä¢	Italy test set
Each evaluation:
	‚Ä¢	Reuses EpisodeSampler with the corresponding split.
	‚Ä¢	Repeats episodic sampling for multiple seeds to get stable metrics.
	‚Ä¢	Uses inner_adapt_params to adapt to the support set for each episode.
	‚Ä¢	Computes:
	‚Ä¢	Regression metrics:
	‚Ä¢	MAE
	‚Ä¢	RMSE
	‚Ä¢	R¬≤
	‚Ä¢	Bland‚ÄìAltman statistics (mean difference, limits of agreement)
	‚Ä¢	Classification metrics (using Hb threshold, e.g. anemia vs non-anemia):
	‚Ä¢	Accuracy
	‚Ä¢	Sensitivity
	‚Ä¢	Specificity
Results are stored in dataframes and sometimes saved as:
	‚Ä¢	metrics_india_val_single_seed.xlsx / .csv
	‚Ä¢	metrics_italy_test_single_seed.xlsx / .csv
(Exact filenames depend on the final cells in the notebook.)

10. How to Run the Notebook (Step-by-Step)
	1.	Open the notebook Anemia_regression_V1-3.ipynb.
	2.	(If in Colab):
	‚Ä¢	Run the first cell to mount Google Drive:‚Ä®from google.colab import drive
	‚Ä¢	drive.mount('/content/drive')
	‚Ä¢	
	3.	If running locally, you can comment out or delete this cell.
	4.	Set your data path:
	‚Ä¢	Go to the ‚ÄúSet Dataset Paths‚Äù cell.
	‚Ä¢	Change:‚Ä®BASE_DIR  = Path("/content/drive/MyDrive/Anemia_dataset")
	‚Ä¢	‚Ä®to your own path.
	‚Ä¢	Run this cell and confirm the printed checks:
	‚Ä¢	Base dir exists: True
	‚Ä¢	India dir exists: True
	‚Ä¢	Italy dir exists: True
	5.	Run metadata & preprocessing cells:
	‚Ä¢	Load India.xlsx and Italy.xlsx.
	‚Ä¢	Build R/G ratio data frames.
	‚Ä¢	Build patient-level and image-level tables.
	‚Ä¢	Run until df_images and df_train, df_val, df_test are created.
	6.	Run dataset & sampler cells:
	‚Ä¢	Instantiate ConjunctivaImageDataset for train/val/test.
	‚Ä¢	Instantiate EpisodeSampler for each split.
	7.	Run model & MAML cells:
	‚Ä¢	Model definition (FewShotConvNet).
	‚Ä¢	inner_adapt_params.
	‚Ä¢	train_maml.
	8.	Run the training cell (the one where train_maml is called).
	‚Ä¢	Monitor printed metrics per epoch.
	9.	Run evaluation cells for India VAL and Italy TEST episodic evaluation.

11. Common Issues & Tips
	‚Ä¢	Base dir exists: False‚Ä®‚Üí Your BASE_DIR is wrong. Double-check the absolute path.
	‚Ä¢	Excel file not found‚Ä®‚Üí Make sure India.xlsx and Italy.xlsx are in the folders pointed to by INDIA_DIR and ITALY_DIR.
	‚Ä¢	FileNotFoundError in ConjunctivaImageDataset‚Ä®‚Üí Some image_path from df_images does not exist on disk.
	‚Ä¢	Print a few df_images["image_path"].head() and manually check.
	‚Ä¢	Confirm your augmented data folder structure matches what build_image_table_from_augmented expects.
	‚Ä¢	Out-of-memory or slow training‚Ä®‚Üí Reduce:
	‚Ä¢	episodes_per_epoch
	‚Ä¢	n_epochs
	‚Ä¢	inner_steps
	‚Ä¢	or reduce image size in img_transform.
	‚Ä¢	Want to use your own dataset
	‚Ä¢	Ensure you have:
	‚Ä¢	Equivalent metadata columns: patient ID, Hb, label.
	‚Ä¢	A similar directory structure, or update:
	‚Ä¢	build_rg_df
	‚Ä¢	build_image_table_from_augmented
	‚Ä¢	Point BASE_DIR to your dataset root, and adjust folder names if needed.
